# -*- coding: utf-8 -*-
"""–ö—É—Ä—Å–∞—áML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1esXkaKIsFe_LDt6Eq_kL-2kXwfHZfq8x

# –≠—Ç–∞–ø 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ç–∞–ø
"""

!pip install tensorflow-datasets tensorflow-text nltk matplotlib > /dev/null
print("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

import tensorflow as tf
gpus = tf.config.list_physical_devices('GPU')
print(f"GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {len(gpus)}")
if gpus:
    print(f"–¢–∏–ø GPU: T4 (16GB –ø–∞–º—è—Ç–∏)")

"""–ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"""

import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_text as text
import numpy as np
import os
import json
import time
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import warnings
warnings.filterwarnings('ignore')

print("–í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
print(f"TensorFlow version: {tf.__version__}")

"""–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞ —Å–µ—Ç–∞"""

examples, metadata = tfds.load(
    'ted_hrlr_translate/pt_to_en',
    with_info=True,
    as_supervised=True
)

print("–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")

"""–û–±—É—á–∞—é—â–∏–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã"""

train_examples, val_examples = examples['train'], examples['validation']
print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(list(train_examples))} –ø—Ä–∏–º–µ—Ä–æ–≤")
print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(list(val_examples))} –ø—Ä–∏–º–µ—Ä–æ–≤")

"""–ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""

for pt_example, en_example in train_examples.take(2):
    print("üáµüáπ –ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π:", pt_example.numpy().decode('utf-8'))
    print("üá¨üáß –ê–Ω–≥–ª–∏–π—Å–∫–∏–π:", en_example.numpy().decode('utf-8'))
    print("-" * 50)

"""–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""

model_name = 'ted_hrlr_translate_pt_en_converter'

downloaded_path = tf.keras.utils.get_file(
    f'{model_name}.zip',
    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',
    cache_dir='.', cache_subdir='', extract=True
)
print(f"–ê—Ä—Ö–∏–≤ —Å–∫–∞—á–∞–Ω –≤: {downloaded_path}")

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–π –ø—É—Ç—å
!find / -name "*ted_hrlr_translate_pt_en_converter*" -type d 2>/dev/null | head -5

# –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å, –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–º–∞–Ω–¥–æ–π find
extracted_folder_path = '/content/ted_hrlr_translate_pt_en_converter_extracted/ted_hrlr_translate_pt_en_converter'

print(f"–ü—ã—Ç–∞—é—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑: {extracted_folder_path}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø—É—Ç—å, –ø—Ä–µ–∂–¥–µ —á–µ–º –∑–∞–≥—Ä—É–∂–∞—Ç—å
if os.path.exists(extracted_folder_path):
    print("–ü–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é –Ω–∞–π–¥–µ–Ω–∞")
    tokenizers = tf.saved_model.load(extracted_folder_path)
    print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
    print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–≥–æ: {tokenizers.pt.get_vocab_size().numpy()}")
    print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ: {tokenizers.en.get_vocab_size().numpy()}")

else:
    print(f"–û—à–∏–±–∫–∞: –ü–∞–ø–∫–∞ –ø–æ –ø—É—Ç–∏ '{extracted_folder_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    print("–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ /content:")
    !ls -la /content/

"""–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""

sample_pt = "Ol√°, mundo!"
sample_en = "Hello, world!"

print("–ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:")
print(f"–ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π: {sample_pt}")
pt_tokens = tokenizers.pt.tokenize([sample_pt])
print(f"–¢–æ–∫–µ–Ω—ã PT: {pt_tokens.to_list()[0][:10]}...")
print(f"–û–±—Ä–∞—Ç–Ω–æ: {tokenizers.pt.detokenize(pt_tokens).numpy()[0].decode('utf-8')}")

print(f"\n–ê–Ω–≥–ª–∏–π—Å–∫–∏–π: {sample_en}")
en_tokens = tokenizers.en.tokenize([sample_en])
print(f"–¢–æ–∫–µ–Ω—ã EN: {en_tokens.to_list()[0][:10]}...")
print(f"–û–±—Ä–∞—Ç–Ω–æ: {tokenizers.en.detokenize(en_tokens).numpy()[0].decode('utf-8')}")

"""–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±–∞—Ç—á–µ–π"""

def prepare_batch(pt, en):
    pt = tokenizers.pt.tokenize(pt)     # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π
    pt = pt.to_tensor()                  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä

    en = tokenizers.en.tokenize(en)     # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
    en = en.to_tensor()                  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä

    # –î–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞: —Å–¥–≤–∏–≥ –Ω–∞ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω
    en_inputs = en[:, :-1]              # –í—Ö–æ–¥ –¥–µ–∫–æ–¥–µ—Ä–∞ (–±–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞)
    en_labels = en[:, 1:]               # –¶–µ–ª—å –¥–µ–∫–æ–¥–µ—Ä–∞ (–±–µ–∑ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞)

    return (pt, en_inputs), en_labels

def make_batches(ds, buffer_size=20000, batch_size=32):
    return (
        ds
        .shuffle(buffer_size)
        .batch(batch_size)
        .map(prepare_batch, tf.data.AUTOTUNE)
        .prefetch(tf.data.AUTOTUNE)
    )

print("–§—É–Ω–∫—Ü–∏–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã")

"""–°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –¢4"""

BATCH_SIZE = 64
BUFFER_SIZE = 20000

print(f"–°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
print(f"  BATCH_SIZE = {BATCH_SIZE} (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è T4)")
print(f"  BUFFER_SIZE = {BUFFER_SIZE}")

train_batches = make_batches(train_examples, BUFFER_SIZE, BATCH_SIZE)
val_batches = make_batches(val_examples, BUFFER_SIZE, BATCH_SIZE)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
for (pt, en), en_labels in train_batches.take(1):
    print(f"\n–ë–∞—Ç—á–∏ —Å–æ–∑–¥–∞–Ω—ã")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–≥–æ: {pt.shape}")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ (–≤—Ö–æ–¥): {en.shape}")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ (—Ü–µ–ª—å): {en_labels.shape}")
    print(f"–ü—Ä–∏–º–µ—Ä: {pt.shape[1]} —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —è–∑—ã–∫–µ-–∏—Å—Ç–æ—á–Ω–∏–∫–µ")

"""–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""

folders = [
    'transformer_checkpoints',
    'saved_models',
    'training_results',
    'attention_viz',
    'translation_examples'
]

for folder in folders:
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f" –°–æ–∑–¥–∞–Ω–∞: {folder}/")
    else:
        print(f" –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {folder}/")

print("\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –≥–æ—Ç–æ–≤–∞")

"""–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞"""

config = {
    'batch_size': BATCH_SIZE,
    'vocab_size_pt': int(tokenizers.pt.get_vocab_size().numpy()),  # <-- –î–æ–±–∞–≤–ª—è–µ–º int()
    'vocab_size_en': int(tokenizers.en.get_vocab_size().numpy()),  # <-- –î–æ–±–∞–≤–ª—è–µ–º int()
    'dataset_size_train': len(list(train_examples)),
    'dataset_size_val': len(list(val_examples)),
    'gpu_type': 'NVIDIA T4 (16GB)',
    'tensorflow_version': tf.__version__,
    'project_name': 'Portuguese to English Transformer',
    'date_created': time.strftime("%Y-%m-%d %H:%M:%S")
}

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
with open('project_config.json', 'w', encoding='utf-8') as f:
    json.dump(config, f, indent=2, ensure_ascii=False)

print("–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ü–†–û–ï–ö–¢–ê:")
for key, value in config.items():
    print(f"{key:25}: {value}")

print("\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'project_config.json'")

"""# –≠—Ç–∞–ø 2. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä

Positional Encoding (–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
"""

class PositionalEncoding(tf.keras.layers.Layer):
    """
    –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Ä—è–¥–∫–µ —Å–ª–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç.
    """
    def __init__(self, position, d_model):
        super(PositionalEncoding, self).__init__()
        self.position = position
        self.d_model = d_model

        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞—Ä–∞–Ω–µ–µ
        pos_encoding = np.zeros((position, d_model))

        # positions.shape: (position, 1)
        positions = np.arange(position)[:, np.newaxis]

        # div_term.shape: (d_model//2,)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º—É–ª—É –∏–∑ —Å—Ç–∞—Ç—å–∏: PE(pos, 2i) = sin(pos/10000^(2i/d_model))
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∏–Ω—É—Å –∫ —á–µ—Ç–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º
        pos_encoding[:, 0::2] = np.sin(positions * div_term)
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—Å–∏–Ω—É—Å –∫ –Ω–µ—á–µ—Ç–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º
        pos_encoding[:, 1::2] = np.cos(positions * div_term)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–∞—Ç—á–∞
        pos_encoding = pos_encoding[np.newaxis, ...]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –≤–µ—Å —Å–ª–æ—è (–Ω–µ –æ–±—É—á–∞–µ–º—ã–π)
        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)

    def call(self, inputs):
        """
        inputs: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, d_model)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: inputs + –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        """
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ seq_len –ø–æ–∑–∏—Ü–∏–π
        seq_len = tf.shape(inputs)[1]
        return inputs + self.pos_encoding[:, :seq_len, :]

# –¢–µ—Å—Ç–∏—Ä—É–µ–º PositionalEncoding
print("–¢–µ—Å—Ç Positional Encoding")
d_model = 512  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ)
position = 1000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

pos_encoding_layer = PositionalEncoding(position, d_model)

# –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –±–∞—Ç—á–∞ –∏–∑ 2 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª–∏–Ω–æ–π 10)
test_input = tf.random.uniform((2, 10, d_model))
output = pos_encoding_layer(test_input)

print(f"Positional Encoding —Ä–∞–±–æ—Ç–∞–µ—Ç")
print(f"   –í—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {test_input.shape}")
print(f"   –í—ã—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {output.shape}")
print(f"   –¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: min={tf.reduce_min(output):.4f}, max={tf.reduce_max(output):.4f}")

"""Multi-Head Attention (–ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)"""

def scaled_dot_product_attention(query, key, value, mask):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ scaled dot-product attention.
    query, key, value: —Ç–µ–Ω–∑–æ—Ä—ã —Ñ–æ—Ä–º—ã (..., seq_len_q, d_model)
    mask: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., seq_len_q, seq_len_k)
    """
    # 1. –£–º–Ω–æ–∂–∞–µ–º query –Ω–∞ key
    matmul_qk = tf.matmul(query, key, transpose_b=True)

    # 2. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
    dk = tf.cast(tf.shape(key)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

    # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if mask is not None:
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ–∑–∏—Ü–∏—è–º
        scaled_attention_logits += (mask * -1e9)

    # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)

    # 5. –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ value
    output = tf.matmul(attention_weights, value)

    return output, attention_weights

class MultiHeadAttention(tf.keras.layers.Layer):
    """
    –°–ª–æ–π –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è.
    –†–∞–∑–¥–µ–ª—è–µ—Ç d_model –Ω–∞ h –≥–æ–ª–æ–≤, –ø—Ä–∏–º–µ–Ω—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∫–∞–∂–¥–æ–π, –∑–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç.
    """
    def __init__(self, d_model, num_heads, name="multi_head_attention"):
        super(MultiHeadAttention, self).__init__(name=name)
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % num_heads == 0, "d_model –¥–æ–ª–∂–Ω–æ –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_heads"

        self.depth = d_model // num_heads  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã

        # –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è query, key, value
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        # –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–∞ (num_heads, depth).
        –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫ —Ñ–æ—Ä–º–µ (batch_size, num_heads, seq_len, depth)
        """
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        # 1. –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        q = self.wq(q)  # (batch_size, seq_len_q, d_model)
        k = self.wk(k)  # (batch_size, seq_len_k, d_model)
        v = self.wv(v)  # (batch_size, seq_len_v, d_model)

        # 2. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –≥–æ–ª–æ–≤—ã
        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)

        # 3. –í—ã—á–∏—Å–ª—è–µ–º scaled dot-product attention
        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask
        )  # scaled_attention: (batch_size, num_heads, seq_len_q, depth)

        # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥–æ–ª–æ–≤—ã
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(scaled_attention,
                                      (batch_size, -1, self.d_model))

        # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        output = self.dense(concat_attention)

        return output, attention_weights

# –¢–µ—Å—Ç–∏—Ä—É–µ–º MultiHeadAttention
print("\n–¢–µ—Å—Ç Multi-Head Attention")
num_heads = 8

# –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
batch_size = 2
seq_len = 10
test_q = tf.random.uniform((batch_size, seq_len, d_model))
test_k = tf.random.uniform((batch_size, seq_len, d_model))
test_v = tf.random.uniform((batch_size, seq_len, d_model))

# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è
mha = MultiHeadAttention(d_model, num_heads)
output, attention_weights = mha(test_v, test_k, test_q, mask=None)

print(f"Multi-Head Attention —Ä–∞–±–æ—Ç–∞–µ—Ç")
print(f"   Query —Ñ–æ—Ä–º–∞: {test_q.shape}")
print(f"   Output —Ñ–æ—Ä–º–∞: {output.shape}")
print(f"   Attention weights —Ñ–æ—Ä–º–∞: {attention_weights.shape}")

"""–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Encoder –∏ Decoder —Å–ª–æ–µ–≤"""

def point_wise_feed_forward_network(d_model, dff):
    """
    –¢–æ—á–µ—á–Ω–∞—è feed-forward —Å–µ—Ç—å (–ø–æ-—ç–ª–µ–º–µ–Ω—Ç–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏).
    –°–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π ReLU –º–µ–∂–¥—É –Ω–∏–º–∏.
    """
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),  # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ)
        tf.keras.layers.Dense(d_model)  # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π (–ø—Ä–æ–µ–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ d_model)
    ])

class EncoderLayer(tf.keras.layers.Layer):
    """–û–¥–∏–Ω —Å–ª–æ–π Encoder'–∞: Self-Attention + Feed Forward"""
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(EncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        # Layer Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        # 1. Self-Attention —Å residual connection
        attn_output, _ = self.mha(x, x, x, mask)  # Self-attention: Q=K=V=x
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)  # Residual connection

        # 2. Feed Forward Network —Å residual connection
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection

        return out2

class DecoderLayer(tf.keras.layers.Layer):
    """–û–¥–∏–Ω —Å–ª–æ–π Decoder'–∞: Masked Self-Attention + Encoder-Decoder Attention + FFN"""
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(DecoderLayer, self).__init__()

        # Masked Self-Attention (–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ)
        self.mha1 = MultiHeadAttention(d_model, num_heads)

        # Encoder-Decoder Attention (–≤–Ω–∏–º–∞–Ω–∏–µ –∫ –≤—ã—Ö–æ–¥—É encoder'–∞)
        self.mha2 = MultiHeadAttention(d_model, num_heads)

        # Feed Forward Network
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        # Layer Normalization
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        # Dropout
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        # enc_output.shape: (batch_size, input_seq_len, d_model)

        # 1. Masked Self-Attention (—Å look_ahead_mask)
        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)  # Residual connection

        # 2. Encoder-Decoder Attention (—Å padding_mask)
        # Q –∏–∑ decoder, K –∏ V –∏–∑ encoder
        attn2, attn_weights_block2 = self.mha2(
            enc_output, enc_output, out1, padding_mask
        )
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)  # Residual connection

        # 3. Feed Forward Network
        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)  # Residual connection

        return out3, attn_weights_block1, attn_weights_block2

# –¢–µ—Å—Ç–∏—Ä—É–µ–º EncoderLayer –∏ DecoderLayer
print("\n–¢–µ—Å—Ç EncoderLayer –∏ DecoderLayer")
dff = 2048  # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è FFN (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ)
dropout_rate = 0.1

# –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
batch_size = 2
seq_len = 10
test_encoder_input = tf.random.uniform((batch_size, seq_len, d_model))
test_decoder_input = tf.random.uniform((batch_size, seq_len, d_model))

# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–∏
encoder_layer = EncoderLayer(d_model, num_heads, dff, dropout_rate)
decoder_layer = DecoderLayer(d_model, num_heads, dff, dropout_rate)

# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ encoder
encoder_output = encoder_layer(test_encoder_input, training=True, mask=None)

# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ decoder
decoder_output, attn1, attn2 = decoder_layer(
    test_decoder_input, encoder_output, training=True,
    look_ahead_mask=None, padding_mask=None
)

print(f"EncoderLayer —Ä–∞–±–æ—Ç–∞–µ—Ç")
print(f"   –í—Ö–æ–¥ encoder: {test_encoder_input.shape}")
print(f"   –í—ã—Ö–æ–¥ encoder: {encoder_output.shape}")

print(f"\nDecoderLayer —Ä–∞–±–æ—Ç–∞–µ—Ç")
print(f"   –í—Ö–æ–¥ decoder: {test_decoder_input.shape}")
print(f"   –í—ã—Ö–æ–¥ decoder: {decoder_output.shape}")
print(f"   Attention weights 1: {attn1.shape}")
print(f"   Attention weights 2: {attn2.shape}")

"""–°–±–æ—Ä–∫–∞ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ Transformer"""

class Encoder(tf.keras.layers.Layer):
    """–ü–æ–ª–Ω—ã–π Encoder: embedding + positional encoding + N —Å–ª–æ–µ–≤ encoder"""
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 maximum_position_encoding, rate=0.1):
        super(Encoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        # Embedding —Å–ª–æ–π (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã)
        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)

        # Positional Encoding
        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)

        # –°—Ç–µ–∫ —Å–ª–æ–µ–≤ Encoder
        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)
                          for _ in range(num_layers)]

        # Dropout –ø–æ—Å–ª–µ embedding + positional encoding
        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, training=False, mask=None):  # <-- –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        seq_len = tf.shape(x)[1]

        # 1. –î–æ–±–∞–≤–ª—è–µ–º embedding
        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º

        # 2. –î–æ–±–∞–≤–ª—è–µ–º positional encoding
        x = self.pos_encoding(x)

        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º dropout
        x = self.dropout(x, training=training)

        # 4. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ encoder
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training=training, mask=mask)

        return x  # (batch_size, input_seq_len, d_model)

class Decoder(tf.keras.layers.Layer):
    """–ü–æ–ª–Ω—ã–π Decoder: embedding + positional encoding + N —Å–ª–æ–µ–≤ decoder"""
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,
                 maximum_position_encoding, rate=0.1):
        super(Decoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        # Embedding —Å–ª–æ–π
        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)

        # Positional Encoding
        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)

        # –°—Ç–µ–∫ —Å–ª–æ–µ–≤ Decoder
        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)
                          for _ in range(num_layers)]

        # Dropout –ø–æ—Å–ª–µ embedding + positional encoding
        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):  # <-- –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        seq_len = tf.shape(x)[1]
        attention_weights = {}  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

        # 1. –î–æ–±–∞–≤–ª—è–µ–º embedding
        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º

        # 2. –î–æ–±–∞–≤–ª—è–µ–º positional encoding
        x = self.pos_encoding(x)

        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º dropout
        x = self.dropout(x, training=training)

        # 4. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ decoder
        for i in range(self.num_layers):
            x, block1, block2 = self.dec_layers[i](
                x, enc_output, training=training,
                look_ahead_mask=look_ahead_mask,
                padding_mask=padding_mask
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            attention_weights[f'decoder_layer{i+1}_block1'] = block1
            attention_weights[f'decoder_layer{i+1}_block2'] = block2

        return x, attention_weights

class Transformer(tf.keras.Model):
    """–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å Transformer"""
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 target_vocab_size, pe_input, pe_target, rate=0.1):
        super(Transformer, self).__init__()

        # Encoder
        self.encoder = Encoder(num_layers, d_model, num_heads, dff,
                              input_vocab_size, pe_input, rate)

        # Decoder
        self.decoder = Decoder(num_layers, d_model, num_heads, dff,
                              target_vocab_size, pe_target, rate)

        # –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤)
        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inputs, training=False):
        # inputs: –∫–æ—Ä—Ç–µ–∂ (inp, tar)
        inp, tar = inputs

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —Å–æ–∑–¥–∞–µ–º –º–∞—Å–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏
        enc_padding_mask = self.create_padding_mask(inp)

        # look_ahead_mask –¥–ª—è tar (–≤—Ö–æ–¥–∞ –¥–µ–∫–æ–¥–µ—Ä–∞)
        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])

        # padding_mask –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ, —á—Ç–æ –∏ –¥–ª—è encoder)
        dec_padding_mask = self.create_padding_mask(inp)

        # 1. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ encoder
        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)

        # 2. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ decoder
        dec_output, attention_weights = self.decoder(
            tar, enc_output, training=training,
            look_ahead_mask=look_ahead_mask,
            padding_mask=dec_padding_mask
        )

        # 3. –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        final_output = self.final_layer(dec_output)

        return final_output, attention_weights

    def create_padding_mask(self, seq):
        """–°–æ–∑–¥–∞–µ—Ç –º–∞—Å–∫—É –¥–ª—è padding —Ç–æ–∫–µ–Ω–æ–≤ (–∑–Ω–∞—á–µ–Ω–∏–µ 1 –¥–ª—è padding)"""
        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)

    def create_look_ahead_mask(self, size):
        """–°–æ–∑–¥–∞–µ—Ç –º–∞—Å–∫—É –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ –≤ decoder"""
        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
        return mask  # (size, size)

"""–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏"""

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ, –Ω–æ —É–º–µ–Ω—å—à–µ–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∞)
num_layers = 4  # –í –æ—Ä–∏–≥–∏–Ω–∞–ª–µ: 6
input_vocab_size = config['vocab_size_pt']  # –ë–µ—Ä–µ–º –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
target_vocab_size = config['vocab_size_en']
pe_input = 1000
pe_target = 1000

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=input_vocab_size,
    target_vocab_size=target_vocab_size,
    pe_input=pe_input,
    pe_target=pe_target,
    rate=dropout_rate
)

# –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
temp_input = tf.random.uniform((BATCH_SIZE, 10), maxval=input_vocab_size, dtype=tf.int64)
temp_target = tf.random.uniform((BATCH_SIZE, 12), maxval=target_vocab_size, dtype=tf.int64)

# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å (–û–ë–†–ê–¢–ò–¢–ï –í–ù–ò–ú–ê–ù–ò–ï: training –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –∫–∞–∫ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç)
fn_out, attention_weights = transformer((temp_input, temp_target), training=True)

print(f"–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å Transformer —Å–æ–∑–¥–∞–Ω–∞")
print(f"   –í—Ö–æ–¥ encoder (–ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π): {temp_input.shape}")
print(f"   –í—Ö–æ–¥ decoder (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π): {temp_target.shape}")
print(f"   –í—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ (–ª–æ–≥–∏—Ç—ã): {fn_out.shape}")
print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {len(attention_weights)}")

# –ü–æ–∫–∞–∂–µ–º —Ñ–æ—Ä–º—É –ø–µ—Ä–≤–æ–≥–æ –±–ª–æ–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
first_key = list(attention_weights.keys())[0]
print(f"   –§–æ—Ä–º–∞ {first_key}: {attention_weights[first_key].shape}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
total_params = transformer.count_params()
print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏:")
print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (d_model): {d_model}")
print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {num_layers}")
print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {num_heads}")

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤ —Ä–µ–∂–∏–º–µ inference (–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è)
print(f"\n–¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ inference (training=False):")
fn_out_inference, _ = transformer((temp_input, temp_target), training=False)
print(f"   –í—ã—Ö–æ–¥ –≤ inference —Ä–µ–∂–∏–º–µ: {fn_out_inference.shape}")

"""# –≠—Ç–∞–ø 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä

–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
"""

# 1. –ö–∞—Å—Ç–æ–º–Ω—ã–π schedule –¥–ª—è learning rate (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ)
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π schedule –¥–ª—è learning rate –ø–æ —Ñ–æ—Ä–º—É–ª–µ –∏–∑ —Å—Ç–∞—Ç—å–∏ "Attention Is All You Need":
    lr = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))
    """
    def __init__(self, d_model, warmup_steps=4000):
        super().__init__()
        self.d_model = tf.cast(d_model, tf.float32)
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
        return lr

# 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è schedule
print("\n1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ learning rate schedule")
learning_rate = CustomSchedule(d_model)
optimizer = tf.keras.optimizers.Adam(
    learning_rate,
    beta_1=0.9,
    beta_2=0.98,
    epsilon=1e-9
)

print(f"   –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam —Å CustomSchedule")
print(f"   –ù–∞—á–∞–ª—å–Ω—ã–π learning rate: {learning_rate(0).numpy():.6f}")
print(f"   Learning rate –ø–æ—Å–ª–µ 4000 —à–∞–≥–æ–≤: {learning_rate(4000).numpy():.6f}")

# –ì—Ä–∞—Ñ–∏–∫ learning rate
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))
steps = range(0, 20000, 100)
lrs = [learning_rate(step).numpy() for step in steps]
plt.plot(steps, lrs)
plt.xlabel('–®–∞–≥ –æ–±—É—á–µ–Ω–∏—è')
plt.ylabel('Learning Rate')
plt.title('Custom Learning Rate Schedule')
plt.grid(True)
plt.savefig('training_results/learning_rate_schedule.png')
plt.show()
print("–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'training_results/learning_rate_schedule.png'")

# 3. –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –º–µ—Ç—Ä–∏–∫–∏
print("\n2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –º–µ—Ç—Ä–∏–∫")

# –ú–∞—Å–∫–∞ –¥–ª—è padding —Ç–æ–∫–µ–Ω–æ–≤ (–Ω–µ —É—á–∏—Ç—ã–≤–∞–µ–º –∏—Ö –≤ loss)
def loss_function(real, pred):
    """
    –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º padding —Ç–æ–∫–µ–Ω–æ–≤
    real: —Ü–µ–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (batch_size, seq_len)
    pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (batch_size, seq_len, vocab_size)
    """
    # 1. sparse_categorical_crossentropy –¥–ª—è —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True, reduction='none'
    )

    # 2. –í—ã—á–∏—Å–ª—è–µ–º loss –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    loss = loss_object(real, pred)

    # 3. –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É (0 –¥–ª—è padding —Ç–æ–∫–µ–Ω–æ–≤)
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    mask = tf.cast(mask, dtype=loss.dtype)

    # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
    loss = loss * mask

    # 5. –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –Ω–µ–Ω—É–ª–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
    return tf.reduce_sum(loss) / tf.reduce_sum(mask)

# –ú–µ—Ç—Ä–∏–∫–∏
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
    name='train_accuracy'
)

print(f"   –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: Sparse Categorical Crossentropy —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º")
print(f"   –ú–µ—Ç—Ä–∏–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏: Sparse Categorical Accuracy")

# 4. –ß–µ–∫–ø–æ–∏–Ω—Ç—ã
print("\n3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤")

checkpoint_path = "transformer_checkpoints/transformer"

ckpt = tf.train.Checkpoint(
    transformer=transformer,
    optimizer=optimizer
)

ckpt_manager = tf.train.CheckpointManager(
    ckpt, checkpoint_path, max_to_keep=5
)

# –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã - –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π
if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: {ckpt_manager.latest_checkpoint}")
else:
    print("–ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")

"""–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""

# 5. –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ
print("\n4. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è...")

@tf.function
def train_step(inp, tar):
    """
    –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ
    inp: –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    tar: –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞ 1 —Ç–æ–∫–µ–Ω)
    """
    # tar_inp –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –≤—Ö–æ–¥ –≤ decoder
    # tar_real –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ —Ü–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    with tf.GradientTape() as tape:
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        predictions, _ = transformer((inp, tar_inp), training=True)

        # –í—ã—á–∏—Å–ª—è–µ–º loss
        loss = loss_function(tar_real, predictions)

    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    train_loss(loss)
    train_accuracy(tar_real, predictions)

    return loss

# 6. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
def translate(sentence, max_length=40):
    """
    –†–∞–±–æ—á–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç int64)
    encoder_input_tokens = tokenizers.pt.tokenize([sentence])  # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç RaggedTensor
    encoder_input = encoder_input_tokens.to_tensor()  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dense tensor

    print(f"Debug: encoder_input shape={encoder_input.shape}, dtype={encoder_input.dtype}")

    # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º START –∏ END —Ç–æ–∫–µ–Ω—ã
    vocab_size = tokenizers.en.get_vocab_size().numpy()
    start_token = vocab_size - 2  # –û–±—ã—á–Ω–æ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω
    end_token = vocab_size - 1    # –û–±—ã—á–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω

    print(f"Debug: vocab_size={vocab_size}, start_token={start_token}, end_token={end_token}")

    # 3. –ù–∞—á–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–ª—è decoder ([START]) - –í–ê–ñ–ù–û: int64
    decoder_input = tf.constant([[start_token]], dtype=tf.int64)

    # 4. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    generated_tokens = []

    for i in range(max_length):
        # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f"Debug: —à–∞–≥ {i}, decoder_input shape={decoder_input.shape}, dtype={decoder_input.dtype}")

        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
        try:
            predictions, _ = transformer(
                (encoder_input, decoder_input),
                training=False
            )
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä—è–º–æ–º –ø—Ä–æ—Ö–æ–¥–µ: {e}")
            break

        # 5. –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω
        # predictions shape: (batch_size, seq_len, vocab_size)
        next_token_logits = predictions[:, -1, :]  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ–∑–∏—Ü–∏—é
        next_token_id = tf.argmax(next_token_logits, axis=-1)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Python int
        next_token_id_val = next_token_id.numpy()[0]
        print(f"Debug: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω —Ç–æ–∫–µ–Ω {next_token_id_val}")

        # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ END —Ç–æ–∫–µ–Ω
        if next_token_id_val == end_token:
            print("Debug: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç END —Ç–æ–∫–µ–Ω")
            break

        # 7. –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –≤ —Å–ø–∏—Å–æ–∫
        generated_tokens.append(next_token_id_val)

        # 8. –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ö–æ–¥ decoder (–¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω)
        # –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∏–ø int64
        next_token_tensor = tf.constant([[next_token_id_val]], dtype=tf.int64)
        decoder_input = tf.concat([decoder_input, next_token_tensor], axis=-1)

    # 9. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç
    if generated_tokens:
        # –í–ê–ñ–ù–û: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int64
        output_tokens = tf.constant([generated_tokens], dtype=tf.int64)
        translated_text = tokenizers.en.detokenize(output_tokens)
        translated_text = translated_text.numpy()[0].decode('utf-8')
    else:
        translated_text = ""

    return translated_text

print("–§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å–æ–∑–¥–∞–Ω—ã")

# 7. –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ –æ–±—É—á–µ–Ω–∏—è
print("\n5. –¢–µ—Å—Ç–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–æ –æ–±—É—á–µ–Ω–∏—è:")

# –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –µ—Å—Ç—å –≤ –Ω–∞—à–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:")
print(f"  –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è EN: {tokenizers.en.get_vocab_size().numpy()}")
print(f"  –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è PT: {tokenizers.pt.get_vocab_size().numpy()}")

# –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º lookup –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–∏–ø–∞—Ö
print("\n–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ lookup:")
test_tokens = tf.constant([[1, 2, 3]], dtype=tf.int64)  # <-- –í–ê–ñ–ù–û: int64
try:
    result = tokenizers.en.lookup(test_tokens)
    print(f"  lookup —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ç–µ–Ω–∑–æ—Ä–µ: {result}")
except Exception as e:
    print(f"  –û—à–∏–±–∫–∞ lookup: {e}")

"""–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""

print("="*60)
print("–≠–¢–ê–ü 3: –û–ë–£–ß–ï–ù–ò–ï –ë–ï–ó –ü–ï–†–ï–ü–û–õ–ù–ï–ù–ò–Ø –ü–ê–ú–Ø–¢–ò")
print("="*60)

# 1. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
print("\n1. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫...")

# –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π BATCH_SIZE
BATCH_SIZE = 16
print(f"   –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {BATCH_SIZE}")

# –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è prepare_batch
def prepare_batch_simple(pt, en):
    """–ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –ø–∞–¥–¥–∏–Ω–≥–∞ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã"""
    pt = tokenizers.pt.tokenize(pt)
    en = tokenizers.en.tokenize(en)

    pt = pt.to_tensor()
    en = en.to_tensor()

    en_inputs = en[:, :-1]
    en_labels = en[:, 1:]

    return (pt, en_inputs), en_labels

def make_batches_safe(ds, buffer_size=10000, batch_size=BATCH_SIZE):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –û–ì–†–ê–ù–ò–ß–ï–ù–ù–´–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π"""
    return (
        ds
        .shuffle(buffer_size)
        .batch(batch_size)
        .map(prepare_batch_simple, tf.data.AUTOTUNE)
        .prefetch(tf.data.AUTOTUNE)
    )

# –°–æ–∑–¥–∞—ë–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –±–∞—Ç—á–∏
train_batches = make_batches_safe(train_examples)
val_batches = make_batches_safe(val_examples)

# –í–ê–ñ–ù–û: –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π
total_batches = len(list(train_examples.batch(BATCH_SIZE)))
print(f"   –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ: {total_batches}")
print("   ‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –±–∞—Ç—á–∏ —Å–æ–∑–¥–∞–Ω—ã")

# 2. –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è train_step
print("\n2. –°–æ–∑–¥–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ train_step...")

@tf.function(reduce_retracing=True)
def train_step_safe(inp, tar, tar_labels):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        with tf.GradientTape() as tape:
            predictions, _ = transformer((inp, tar), training=True)
            loss = loss_function(tar_labels, predictions)

        gradients = tape.gradient(loss, transformer.trainable_variables)
        if gradients[0] is not None:
            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

        train_loss(loss)
        train_accuracy(tar_labels, predictions)

        return loss
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ train_step: {str(e)[:100]}")
        return tf.constant(0.0, dtype=tf.float32)

print("   ‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")

# 3. –ó–ê–ü–£–°–ö –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
print("\n" + "="*60)
print("3. –ó–ê–ü–£–°–ö –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
print("="*60)

EPOCHS = 10
print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {EPOCHS}")
print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {BATCH_SIZE}")
print(f"   –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∂–∏–º: –í–ö–õ")

import time
from tqdm import tqdm

history = {'loss': [], 'accuracy': []}

print(f"\n–ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {EPOCHS} —ç–ø–æ—Ö...")

# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
for epoch in range(EPOCHS):
    start = time.time()
    train_loss.reset_state()
    train_accuracy.reset_state()

    print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{EPOCHS}")

    batch_counter = 0
    successful_batches = 0

    # –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º .take() –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
    # –ò–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≤ —ç–ø–æ—Ö–µ

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º .take() —Å –æ–±—â–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –±–∞—Ç—á–µ–π
    for batch_data in train_batches.take(total_batches):
        try:
            (inp, tar), tar_labels = batch_data
            batch_counter += 1

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 –±–∞—Ç—á–µ–π
            if batch_counter % 50 == 0:
                current_loss = train_loss.result().numpy()
                current_acc = train_accuracy.result().numpy()
                print(f"   –ë–∞—Ç—á {batch_counter}/{total_batches}, Loss: {current_loss:.4f}, Acc: {current_acc:.4f}")

            # –®–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            loss_val = train_step_safe(inp, tar, tar_labels)
            successful_batches += 1

            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 100 –±–∞—Ç—á–µ–π
            if batch_counter % 100 == 0:
                import gc
                gc.collect()

        except Exception as e:
            print(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –±–∞—Ç—á {batch_counter}: {str(e)[:80]}")
            continue

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–æ—Ö–∏
    epoch_loss = train_loss.result().numpy()
    epoch_acc = train_accuracy.result().numpy()
    history['loss'].append(epoch_loss)
    history['accuracy'].append(epoch_acc)

    print(f"\n   –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ {epoch + 1}:")
    print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {successful_batches}/{batch_counter}")
    print(f"   - Loss: {epoch_loss:.4f}")
    print(f"   - Accuracy: {epoch_acc:.4f}")
    print(f"   - –í—Ä–µ–º—è: {time.time() - start:.2f} —Å–µ–∫—É–Ω–¥")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
    if ckpt_manager:
        ckpt_save_path = ckpt_manager.save()
        print(f"   üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {ckpt_save_path}")

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥
    try:
        test_translation = translate_working("ol√°")
        print(f"   - –¢–µ—Å—Ç: 'ol√°' ‚Üí '{test_translation}'")
    except Exception as e:
        print(f"   - –¢–µ—Å—Ç: –ø—Ä–æ–ø—É—â–µ–Ω (–æ—à–∏–±–∫–∞: {str(e)[:50]})")

    print("-" * 50)

print("\n" + "="*60)
print("üéâ –ë–ï–ó–û–ü–ê–°–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
print("="*60)

"""–¢–ï–°–¢–´"""

print("="*80)
print("üìä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
print("="*80)

# 1.1 –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
def translate_model(sentence, max_length=50, temperature=1.0):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π
    temperature: 1.0 = –æ–±—ã—á–Ω—ã–π argmax, <1.0 = –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–æ, >1.0 = –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω–æ
    """
    print(f"\nüîç –ü–µ—Ä–µ–≤–æ–¥: '{sentence}'")

    try:
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–∞
        encoder_input = tokenizers.pt.tokenize([sentence]).to_tensor()
        print(f"   –¢–æ–∫–µ–Ω–æ–≤ PT: {encoder_input.shape[1]}")

        # START –∏ END —Ç–æ–∫–µ–Ω—ã
        vocab_size = tokenizers.en.get_vocab_size().numpy()
        start_token = vocab_size - 2  # [START]
        end_token = vocab_size - 1    # [END]

        # –ù–∞—á–∏–Ω–∞–µ–º —Å START —Ç–æ–∫–µ–Ω–∞
        decoder_input = tf.constant([[start_token]], dtype=tf.int64)
        generated_tokens = []
        generated_texts = []

        print(f"   –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")

        for i in range(max_length):
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            predictions, attention_weights = transformer(
                (encoder_input, decoder_input),
                training=False
            )

            # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏—Ç—ã
            logits = predictions[:, -1, :] / temperature

            # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            probabilities = tf.nn.softmax(logits, axis=-1)

            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å argmax –∏–ª–∏ sampling)
            if temperature == 1.0:
                next_token_id = tf.argmax(logits, axis=-1).numpy()[0]
            else:
                # Sampling –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                next_token_id = tf.random.categorical(logits, num_samples=1).numpy()[0, 0]

            # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if i < 3:  # –¢–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 —à–∞–≥–æ–≤
                top_k = 3
                top_values, top_indices = tf.math.top_k(logits[0], k=top_k)

                print(f"   –®–∞–≥ {i}:")
                for j in range(top_k):
                    token_id = top_indices[j].numpy()
                    token_prob = tf.nn.softmax(top_values)[j].numpy()

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–æ–∫–µ–Ω–∞
                    try:
                        token_text = tokenizers.en.lookup(
                            tf.constant([[token_id]], dtype=tf.int64)
                        ).numpy()[0][0].decode('utf-8')
                    except:
                        token_text = f"[token {token_id}]"

                    print(f"     {j+1}. '{token_text}' (id: {token_id}, prob: {token_prob:.4f})")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ END —Ç–æ–∫–µ–Ω
            if next_token_id == end_token:
                print(f"   ‚úì –î–æ—Å—Ç–∏–≥–Ω—É—Ç END —Ç–æ–∫–µ–Ω –Ω–∞ —à–∞–≥–µ {i}")
                break

            generated_tokens.append(next_token_id)

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ö–æ–¥ –¥–µ–∫–æ–¥–µ—Ä–∞
            decoder_input = tf.concat([
                decoder_input,
                tf.constant([[next_token_id]], dtype=tf.int64)
            ], axis=-1)

        # –î–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if generated_tokens:
            output_tensor = tf.constant([generated_tokens], dtype=tf.int64)
            translated_text = tokenizers.en.detokenize(output_tensor)
            translated_text = translated_text.numpy()[0].decode('utf-8')

            print(f"   ‚úì –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(generated_tokens)}")
            return translated_text
        else:
            return "[–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏]"

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")
        return f"[–æ—à–∏–±–∫–∞: {str(e)[:50]}]"

# 1.2 –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
print("\n" + "="*60)
print("1. –ö–ê–ß–ï–°–¢–í–ï–ù–ù–ê–Ø –û–¶–ï–ù–ö–ê –ü–ï–†–ï–í–û–î–ê")
print("="*60)

test_cases = [
    # –ü—Ä–æ—Å—Ç—ã–µ —Ñ—Ä–∞–∑—ã
    ("ol√°", "hello"),
    ("bom dia", "good morning"),
    ("obrigado", "thank you"),
    ("sim", "yes"),
    ("n√£o", "no"),

    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    ("como voc√™ est√°?", "how are you?"),
    ("eu estou bem", "I am fine"),
    ("qual √© o seu nome?", "what is your name?"),
    ("meu nome √© Jo√£o", "my name is Jo√£o"),

    # –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ
    ("eu gosto de aprender portugu√™s", "I like learning Portuguese"),
    ("onde fica a esta√ß√£o de trem?", "where is the train station?"),
    ("quanto custa isso?", "how much does this cost?"),

    # –ò–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)
    ("este √© um exemplo de tradu√ß√£o", "this is an example of translation"),
]

print("–ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã...\n")

results = []
for i, (pt_sentence, expected_en) in enumerate(test_cases):
    print(f"\n–¢–µ—Å—Ç {i+1}/{len(test_cases)}:")
    print(f"üáµüáπ –í—Ö–æ–¥: '{pt_sentence}'")
    print(f"üá¨üáß –û–∂–∏–¥–∞–µ—Ç—Å—è: '{expected_en}'")

    # –ü–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª—å—é
    translated = translate_model(pt_sentence)
    print(f"üá¨üáß –ú–æ–¥–µ–ª—å: '{translated}'")

    # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
    score = 0
    if translated and translated != "[–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏]":
        # –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        expected_lower = expected_en.lower()
        translated_lower = translated.lower()

        if expected_lower in translated_lower:
            score = 1
        elif any(word in translated_lower for word in expected_lower.split()[:2]):
            score = 0.5

    results.append({
        'input': pt_sentence,
        'expected': expected_en,
        'translated': translated,
        'score': score
    })

    print(f"üìä –û—Ü–µ–Ω–∫–∞: {score}/1")
    print("-" * 50)

# 1.3 –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
print("\n" + "="*60)
print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ü–ï–†–ï–í–û–î–ê")
print("="*60)

total_score = sum(r['score'] for r in results)
avg_score = total_score / len(results) if results else 0

print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {len(results)}")
print(f"–û–±—â–∏–π –±–∞–ª–ª: {total_score}/{len(results)}")
print(f"–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: {avg_score:.2%}")

# –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
print("\n–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
for i, r in enumerate(results):
    score_symbol = "‚úÖ" if r['score'] == 1 else "‚ö†Ô∏è" if r['score'] == 0.5 else "‚ùå"
    print(f"{score_symbol} –¢–µ—Å—Ç {i+1}: '{r['input']}' ‚Üí '{r['translated']}'")

print("\n" + "="*60)
print("2. –†–ê–°–ß–Å–¢ BLEU SCORE")
print("="*60)

try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    import nltk

    # –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
    nltk.download('punkt_tab', quiet=True)

    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º BLEU score...")

    # –ë–µ—Ä–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    print("–ë–µ—Ä—ë–º 100 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞...")

    test_samples = []
    for pt_example, en_example in val_examples.take(100):
        pt_text = pt_example.numpy().decode('utf-8')
        en_reference = en_example.numpy().decode('utf-8')

        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å—é
        translation = translate_model(pt_text)

        if translation and translation != "[–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏]":
            test_samples.append({
                'source': pt_text,
                'reference': en_reference,
                'translation': translation
            })

    print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(test_samples)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –í—ã—á–∏—Å–ª—è–µ–º BLEU score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    smoothie = SmoothingFunction().method4
    bleu_scores = []

    print("\n–í—ã—á–∏—Å–ª—è–µ–º BLEU score...")
    for i, sample in enumerate(test_samples[:10]):  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        reference = [sample['reference'].split()]
        candidate = sample['translation'].split()

        try:
            bleu = sentence_bleu(reference, candidate,
                                smoothing_function=smoothie)
            bleu_scores.append(bleu)

            if i < 3:  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"\n–ü—Ä–∏–º–µ—Ä {i+1}:")
                print(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: '{sample['source'][:50]}...'")
                print(f"  –≠—Ç–∞–ª–æ–Ω:   '{sample['reference'][:50]}...'")
                print(f"  –ü–µ—Ä–µ–≤–æ–¥:  '{sample['translation'][:50]}...'")
                print(f"  BLEU:     {bleu:.4f}")

        except Exception as e:
            if i < 3:
                print(f"  BLEU –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω: {str(e)[:50]}")
            continue

    if bleu_scores:
        avg_bleu = sum(bleu_scores) / len(bleu_scores)
        print(f"\nüìä –ò–¢–û–ì–û–í–´–ô BLEU SCORE:")
        print(f"   –°—Ä–µ–¥–Ω–∏–π BLEU: {avg_bleu:.4f}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {min(bleu_scores):.4f}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {max(bleu_scores):.4f}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(bleu_scores)}")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è BLEU score
        print(f"\nüìñ –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:")
        if avg_bleu > 0.3:
            print("   ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞!")
        elif avg_bleu > 0.2:
            print("   üëç –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞")
        elif avg_bleu > 0.1:
            print("   ‚ö†Ô∏è –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–Ω—É–∂–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è)")
        else:
            print("   ‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å BLEU score")

except Exception as e:
    print(f"‚ö†Ô∏è BLEU score –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {str(e)[:100]}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: !pip install nltk")

print("\n" + "="*60)
print("3. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ú–ï–•–ê–ù–ò–ó–ú–ê –í–ù–ò–ú–ê–ù–ò–Ø")
print("="*60)

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

def plot_attention(attention, sentence, predicted_sentence, layer_name="layer1"):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è
    """
    fig = plt.figure(figsize=(10, 10))

    # attention shape: (batch=1, num_heads, target_seq, source_seq)
    attention = attention[0]  # –£–±–∏—Ä–∞–µ–º batch dimension

    for head in range(min(4, attention.shape[0])):  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 4 –≥–æ–ª–æ–≤—ã
        ax = fig.add_subplot(2, 2, head + 1)

        # –ë–µ—Ä–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥–æ–ª–æ–≤—ã
        attn_map = attention[head].numpy()

        # –î–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        if attn_map.shape[0] > 20:
            attn_map = attn_map[:20, :20]
            pred_sent_short = predicted_sentence.split()[:20]
            sent_short = sentence.split()[:20]
        else:
            pred_sent_short = predicted_sentence.split()
            sent_short = sentence.split()

        # –°–æ–∑–¥–∞–µ–º heatmap
        cax = ax.matshow(attn_map, cmap='viridis')
        fig.colorbar(cax, ax=ax)

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Å–∏
        ax.set_xticks(range(len(sent_short)))
        ax.set_yticks(range(len(pred_sent_short)))

        ax.set_xticklabels(sent_short, rotation=90)
        ax.set_yticklabels(pred_sent_short)

        ax.set_xlabel('Source (Portuguese)')
        ax.set_ylabel('Target (English)')
        ax.set_title(f'Attention Head {head + 1}')

    plt.tight_layout()
    plt.savefig(f'attention_plots/attention_{layer_name}.png', dpi=150, bbox_inches='tight')
    plt.show()
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: attention_plots/attention_{layer_name}.png")

# –¢–µ—Å—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
print("–¢–µ—Å—Ç–∏—Ä—É–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –≤–Ω–∏–º–∞–Ω–∏—è...")

test_sentence = "ol√° como voc√™ est√°"
print(f"\n–¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: '{test_sentence}'")

try:
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    encoder_input = tokenizers.pt.tokenize([test_sentence]).to_tensor()

    # START —Ç–æ–∫–µ–Ω
    start_token = tokenizers.en.get_vocab_size().numpy() - 2
    decoder_input = tf.constant([[start_token]], dtype=tf.int64)

    # –ü–æ–ª—É—á–∞–µ–º attention weights
    predictions, attention_weights = transformer(
        (encoder_input, decoder_input),
        training=False
    )

    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ attention –±–ª–æ–∫–æ–≤: {len(attention_weights)}")

    # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–π –±–ª–æ–∫ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è
    first_key = list(attention_weights.keys())[0]
    first_attention = attention_weights[first_key]

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    translated = translate_model(test_sentence)

    if translated and translated != "[–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏]":
        print(f"–ü–µ—Ä–µ–≤–æ–¥ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: '{translated}'")
        plot_attention(first_attention, test_sentence, translated, first_key)
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)[:100]}")

print("\n" + "="*60)
print("4. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –†–ê–ó–ù–´–• –¢–ò–ü–ê–• –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô")
print("="*60)

test_categories = {
    "–í–æ–ø—Ä–æ—Å—ã": [
        "onde fica o banheiro?",
        "que horas s√£o?",
        "como se chama isso em ingl√™s?",
        "voc√™ fala portugu√™s?",
        "quanto custa uma passagem?",
    ],
    "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è": [
        "ol√°, tudo bem?",
        "bom dia, senhor",
        "boa noite, como vai?",
        "prazer em conhec√™-lo",
        "at√© logo",
    ],
    "–ü–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–µ —Ñ—Ä–∞–∑—ã": [
        "eu gosto de caf√©",
        "estou com fome",
        "hoje est√° um dia lindo",
        "preciso de ajuda",
        "onde posso comer?",
    ],
    "–ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ": [
        "a intelig√™ncia artificial est√° mudando o mundo",
        "estou aprendendo programa√ß√£o em python",
        "o futuro da tradu√ß√£o autom√°tica √© promissor",
        "esta √© uma frase longa para testar a capacidade do modelo",
    ]
}

print("–ó–∞–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...\n")

category_results = {}

for category, sentences in test_categories.items():
    print(f"\nüìÅ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
    print("-" * 50)

    cat_scores = []

    for i, sentence in enumerate(sentences[:3]):  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ 3 –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        print(f"\n  –ü—Ä–∏–º–µ—Ä {i+1}: '{sentence}'")

        # –ü–µ—Ä–µ–≤–æ–¥
        translated = translate_model(sentence)
        print(f"  –ü–µ—Ä–µ–≤–æ–¥: '{translated}'")

        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        if translated and len(translated.split()) > 1:
            score = 0.7  # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –∑–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å
        elif translated:
            score = 0.3  # –û–¥–Ω–æ —Å–ª–æ–≤–æ
        else:
            score = 0

        cat_scores.append(score)
        print(f"  –û—Ü–µ–Ω–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏: {score:.1f}/1")

    if cat_scores:
        avg_score = sum(cat_scores) / len(cat_scores)
        category_results[category] = avg_score
        print(f"\n  üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {avg_score:.2f}/1")

print("\n" + "="*60)
print("–ò–¢–û–ì–ò –ö–ê–¢–ï–ì–û–†–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
print("="*60)

for category, score in category_results.items():
    print(f"{category:20}: {score:.2f}/1")